# -*- coding: utf-8 -*-
"""MLP2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LfD11_kkZPyfThp8F-oivIp0iWEv3tfC
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
# %matplotlib inline

# read the data ser
words = open('names.txt','r').read().splitlines()

# Build a vocabulary of characters and a mapping from/to integers
chars = sorted(list(set(''.join(words))))
stoi = {s:i+1 for i,s in enumerate(chars)}
stoi['.'] = 0
itos = {i:s for s,i in stoi.items()}
vocab_size = len(itos)

#data set
blockSize = 3

def build_dataset(words):

  X,Y = [], [] #inputs and targets
  for w in words:
    context = [0] * blockSize
    for ch in w + '.':
      ix = stoi[ch]
      X.append(context)
      Y.append(ix)
      context = context[1:] + [ix] #crop and append

  X = torch.tensor(X)
  Y = torch.tensor(Y)
  return X, Y

import random
random.seed(42)
random.shuffle(words) #randomly shuffle the words
n1 = int(0.8 * len(words))
n2 = int(0.9 * len(words))

Xtr, Ytr = build_dataset(words[:n1])       #80%
Xdev, Ydev = build_dataset(words[n1:n2])   #10%
Xte, Yte = build_dataset(words[n2:])       #10%

n_embed = 10   # The dimensionality of the character embedding vectors
n_hidden = 200 # The number of neurons in the hidden layer

g = torch.Generator().manual_seed(2147483647) #for reproducibility

C = torch.randn((vocab_size,n_embed),              generator = g)
W1 = torch.randn((n_embed * blockSize , n_hidden), generator = g)* (5/3)/ (n_embed * blockSize)**0.5
b1 = torch.randn(n_hidden,                         generator = g)*0.01
W2 = torch.randn((n_hidden,vocab_size),            generator = g)*0.01
b2 = torch.randn(vocab_size,                       generator = g)*0

# ------- Define gains and biases for the batch normalization ---------
bngain=torch.ones((1,n_hidden))
bnbias = torch.zeros((1,n_hidden))

# define running average and std
# initilizations will make sure the initial hidden states are normal
bnmean_running = torch.zeros((1,n_hidden))
bnstd_running = torch.ones((1,n_hidden))

parameters = [C, W1, b1 ,W2, b2, bngain, bnbias]
print(sum(p.nelement() for p in parameters)) #number of parameters in total

for p in parameters:
  p.requires_grad = True

# optimization
max_steps = 200000
batch_size = 50
lossi =[]

for i in range(max_steps):
  #mini batch construct
  ix = torch.randint(0,Xtr.shape[0],(batch_size,))

  #forward pass
  emb = C[Xtr[ix]]
  embcat = emb.view(-1, n_embed * blockSize)
  hpreact = embcat @ W1 + b1

  #------Batch normalization ------
  bnmeani = hpreact.mean(0,keepdims=True)
  bnstdi = hpreact.std(0,keepdims = True)

  hpreact =  (bngain*(hpreact- bnmeani)/bnstdi)-bnbias

  # no gradient calculations for running mean/ std
  with torch.no_grad():
    bnmean_running = 0.999*bnmean_running + 0.001*bnmeani
    bnstd_running = 0.999*bnstd_running + 0.001*bnstdi

  h = torch.tanh(hpreact)
  logits = h @ W2 + b2
  loss = F.cross_entropy(logits, Ytr[ix])
  #backward pass
  for p in parameters:
    p.grad = None
  loss.backward()

  #update
  lr = 0.1 if i<10000 else 0.01
  for p in parameters:
    p.data += -lr*p.grad

  if i%10000 ==0:
    print(f"{i:7d}/ {max_steps:7d}: {loss.item():.4f}")

  lossi.append(loss.log10().item())

plt.plot(lossi)

plt.hist(h.view(-1).tolist(),50)

plt.figure(figsize=(20,10))
plt.imshow(h.abs() > 0.99, cmap = 'gray', interpolation = 'nearest')

#calibarate batch norm at the end of the training.

with torch.no_grad():
  emb = C[Xtr]
  embcat = emb.view(-1, n_embed * blockSize)
  hpreact = embcat @ W1 + b1
  # calculte the mean and std of the entire training data set
  bnmean = hpreact.mean(0,keepdims=True)
  bnstd = hpreact.std(0,keepdims=True)

@torch.no_grad()  # this decorator disables gradient tracking
def split_loss(split):
  x,y = {
      'train': (Xtr,Ytr),
      'val' : (Xdev,Ydev),
      'test': (Xte,Yte)
  }[split]
  emb = C[x]
  embcat = emb.view(-1, n_embed * blockSize)
  hpreact = embcat @ W1 + b1
  hpreact =  bngain*(hpreact-bnmean_running)/bnstd_running-bnbias
  h = torch.tanh(hpreact)
  logits = h @ W2 + b2
  loss = F.cross_entropy(logits, y)
  print(split, f"{loss.item() : .4f}")

split_loss('train')
split_loss('val')

# sample from the distribution
g = torch.Generator().manual_seed(2147483647+10)

for _ in range(10):

  out = []
  context = [0]* blockSize

  while True:

    # forward pass
    emb = C[torch.tensor([context])]
    h = torch.tanh(emb.view(1, -1) @ W1 + b1)
    logits = h @ W2 + b2
    probs = F.softmax(logits, dim=1)

    # sample from the distribution
    ix = torch.multinomial(probs, num_samples=1, generator = g).item()

    #shift the context window
    context = context[1:] + [ix]
    out.append(ix)
    if ix == 0:
      break

  print(''.join(itos[i] for i in  out))

# just some activation functions
import numpy as np
x = np.arange(-3,+3,0.01)
y = [xs if xs>0 else 0 for xs in x ]
plt.plot(x,y)
plt.xlabel('x')
plt.ylabel('y')
plt.grid(color='r', linestyle='-', linewidth=0.1)

# distributions and multiplication of random variables
x = torch.randn(1000,10)
W = torch.randn(10,200)/ 10**0.5
y = x @ W
print(x.mean(),x.std())
print(y.mean(),y.std())

plt.figure(figsize =(20,5))

plt.subplot(121)
plt.hist(x.view(-1).tolist(),50,density = True)

plt.subplot(122)
plt.hist(y.view(-1).tolist(),50, density = True)